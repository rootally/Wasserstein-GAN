{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from collections import defaultdict\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Multiply, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D,Conv2DTranspose\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.backend.common import _EPSILON\n",
    "from keras.utils.generic_utils import Progbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"Calculates the Wasserstein loss for a sample batch.\n",
    "    \n",
    "    The Wasserstein loss function is very simple to calculate. In a standard GAN, the discriminator\n",
    "    has a sigmoid output, representing the probability that samples are real or generated. In Wasserstein\n",
    "    GANs, however, the output is linear with no activation function! Instead of being constrained to [0, 1],\n",
    "    the discriminator wants to make the distance between its output for real and generated samples as large as possible.\n",
    "    The most natural way to achieve this is to label generated samples -1 and real samples 1, instead of the\n",
    "    0 and 1 used in normal GANs, so that multiplying the outputs by the labels will give you the loss immediately.\n",
    "    Note that the nature of this loss means that it can be (and frequently will be) less than 0.\"\"\"\n",
    "    \n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(latent_size):\n",
    "    \"\"\"Creates a generator model and outputs images of size 28x28x1.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=latent_size))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(128 * 7 * 7))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n",
    "    # upsample to (..., 14, 14)\n",
    "    model.add(Conv2DTranspose(128, (5, 5), strides=2, padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    # upsample to (..., 28, 28)\n",
    "    model.add(Conv2DTranspose(64, (5, 5), strides=2, padding='same'))\n",
    "    model.add(LeakyReLU())\n",
    "    # Because we normalized training inputs to lie in the range [-1, 1],\n",
    "    # the tanh function should be used for the output of the generator to ensure its output\n",
    "    # also lies in this range.\n",
    "    model.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "    \n",
    "    latent = Input(shape=(latent_size, ))\n",
    "\n",
    "    # sample label\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    #  MNIST classes\n",
    "    cls = Flatten()(Embedding(10, latent_size, embeddings_initializer ='glorot_uniform')(image_class))\n",
    "\n",
    "    # hadamard product between z-space and a class conditional embedding\n",
    "    h = Multiply()([latent, cls])\n",
    "\n",
    "    fake_image = model(h)\n",
    "\n",
    "    return Model(inputs=[latent, image_class], outputs=fake_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', strides=(2, 2), input_shape=(1, 28, 28)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2D(128, (5, 5), kernel_initializer='he_normal',padding='same', strides=(2, 2)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2D(128, (5, 5), kernel_initializer='he_normal', padding='same', strides=[2, 2]))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    image = Input(shape=(1, 28, 28))\n",
    "    features = model(image)\n",
    "    # first output (name=generation) is whether or not the discriminator\n",
    "    # thinks the image that is being shown is fake, and the second output\n",
    "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
    "    # belongs to.\n",
    "    fake = Dense(1, activation='linear', name='generation')(features)\n",
    "    aux = Dense(10, activation='softmax', name='auxiliary')(features)\n",
    "\n",
    "    return Model(inputs=image, outputs=[fake, aux])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch and latent size taken from the paper\n",
    "nb_epochs = 50\n",
    "batch_size = 100\n",
    "latent_size = 100\n",
    "\n",
    "# Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "# build the discriminator\n",
    "discriminator = make_discriminator()\n",
    "discriminator.compile(optimizer= Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "loss=[wasserstein_loss, 'sparse_categorical_crossentropy'])\n",
    "    \n",
    "    \n",
    "# build the generator\n",
    "generator = make_generator(latent_size)\n",
    "generator.compile(optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
    "                      loss='binary_crossentropy')\n",
    "\n",
    "latent = Input(shape=(latent_size, ))\n",
    "image_class = Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "# get a fake image\n",
    "fake = generator([latent, image_class])\n",
    "# we only want to be able to train generation for the combined model\n",
    "discriminator.trainable = False\n",
    "fake, aux = discriminator(fake)\n",
    "combined = Model(inputs=[latent, image_class], outputs=[fake, aux])\n",
    "\n",
    "combined.compile(optimizer='RMSprop', loss=[wasserstein_loss, 'sparse_categorical_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our mnist data, and force it to be of shape (..., 1, 28, 28) with # range [-1, 1]\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "    \n",
    "nb_train, nb_test = X_train.shape[0], X_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def generate_images(epoch):\n",
    "        # generate some digits to display\n",
    "        noise = np.random.normal(-1, 1, (100, latent_size))\n",
    "        sampled_labels = np.array([[i] * 10 for i in range(10)]).reshape(-1, 1)\n",
    "\n",
    "        # get a batch to display\n",
    "        generated_images = generator.predict([noise, sampled_labels], verbose=0)\n",
    "\n",
    "        # arrange them into a grid\n",
    "        img = (np.concatenate([r.reshape(-1, 28)\n",
    "                               for r in np.split(generated_images, 10)\n",
    "                               ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "        Image.fromarray(img).save('plot_epoch_{0:03d}_generated.png'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "  3/600 [..............................] - ETA: 6401s - dis_loss: 2.2597 - gen_loss: 2.1866"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e0f572eb709e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             gen_loss.append(combined.train_on_batch(\n\u001b[0;32m---> 55\u001b[0;31m                 [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels]))\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTesting for epoch {}:'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "discriminator_train_loss = []\n",
    "generator_train_loss = [] \n",
    "\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "        print('Epoch {} of {}'.format(epoch + 1, nb_epochs))\n",
    "        #No. of batches\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        #Keras Progress bar\n",
    "        progress_bar = Progbar(target=nb_batches)\n",
    "\n",
    "        gen_loss = []\n",
    "        dis_loss = []\n",
    "\n",
    "        for index in range(nb_batches):\n",
    "            if len(gen_loss) + len(dis_loss) > 1:\n",
    "                progress_bar.update(index, values=[('dis_loss',np.mean(np.array(dis_loss),axis=0)[0]), ('gen_loss', np.mean(np.array(gen_loss),axis=0)[0])])\n",
    "            else:\n",
    "                progress_bar.update(index)\n",
    "                \n",
    "            # generate a new batch of noise\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_size))\n",
    "\n",
    "            # get a batch of real images\n",
    "            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            # sample some labels from p_c\n",
    "            sampled_labels = np.random.randint(0, 10, batch_size)\n",
    "\n",
    "            # generate a batch of fake images, using the generated labels as a\n",
    "            # conditioner.\n",
    "            generated_images = generator.predict([noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = np.array([-1] * batch_size + [1] * batch_size)\n",
    "            aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n",
    "\n",
    "            # Training the discriminator network\n",
    "            dis_loss.append(discriminator.train_on_batch(X, [y, aux_y]))\n",
    "\n",
    "            # make new noise. we generate 2 * batch size here such that we have\n",
    "            # the generator optimize over an identical number of images as the\n",
    "            # discriminator\n",
    "            noise = np.random.normal(0, 1, (2 * batch_size, latent_size))\n",
    "            sampled_labels = np.random.randint(0, 10, 2 * batch_size)\n",
    "\n",
    "            # we want to train the genrator to trick the discriminator\n",
    "            # For the generator, we want all the {fake, not-fake} labels to say\n",
    "            # not-fake\n",
    "            trick = -np.ones(2 * batch_size)\n",
    "\n",
    "            gen_loss.append(combined.train_on_batch(\n",
    "                [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels]))\n",
    "\n",
    "        print('\\nTesting for epoch {}:'.format(epoch + 1))\n",
    "        \n",
    "        discriminator_train_loss.append(np.mean(np.array(dis_loss), axis=0))\n",
    "        generator_train_loss.append(np.mean(np.array(gen_loss), axis=0))\n",
    "\n",
    "        \n",
    "        # save weights every epoch\n",
    "        generator.save_weights(\n",
    "            'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "        discriminator.save_weights(\n",
    "            'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n",
    "        \n",
    "        #function to generate image at every epoch!\n",
    "        generate_images(epoch)\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
